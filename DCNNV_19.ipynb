{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "774ada71-4a62-4673-8173-a1180caf311a",
      "metadata": {
        "id": "774ada71-4a62-4673-8173-a1180caf311a"
      },
      "outputs": [],
      "source": [
        "# Imports the libraries\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn as skl\n",
        "from datetime import datetime\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "import pandas as pd\n",
        "import keras\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "# pip3 install adabelief-tf==0.2.0 #Installs the AdaBelief optimizer from https://github.com/juntang-zhuang/Adabelief-Optimizer#2-tensorflow-implementation-eps-of-adabelief-in-tensorflow-is-larger-than-in-pytorch-same-for-adam (nowadays this optimizer is available on TensorFlow Addons)\n",
        "from adabelief_tf import AdaBeliefOptimizer\n",
        "from matplotlib.pyplot import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe6fb65e-4171-4410-8fa6-ae9f244eca22",
      "metadata": {
        "id": "fe6fb65e-4171-4410-8fa6-ae9f244eca22"
      },
      "outputs": [],
      "source": [
        "# Checks if the GPU is available\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f10240-c1ff-4cc4-ae12-06760a150273",
      "metadata": {
        "id": "c8f10240-c1ff-4cc4-ae12-06760a150273"
      },
      "outputs": [],
      "source": [
        "# Path for the training, validation, test, checkpoint(used by the callback to save the model during the training), and logs(to be used by TensorBoard)\n",
        "train_path = '/dataset/train'\n",
        "validation_path = '/dataset/val'\n",
        "test_path = '/dataset/test'\n",
        "checkpoint_path = '/checkpoint_model'\n",
        "logdir = '/tensorboard_logs/'  + datetime.now().strftime(\"%d%m%Y-%H%M%S\")\n",
        "\n",
        "size=128 # image size\n",
        "batch=128 # batch size\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  train_path,\n",
        "  batch_size=batch,\n",
        "  image_size=(size, size),\n",
        "  seed=123\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  validation_path,\n",
        "  batch_size=batch,  \n",
        "  image_size=(size, size),\n",
        "  seed=123\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  test_path,\n",
        "  batch_size=batch,\n",
        "  image_size=(size, size),\n",
        "  seed=123\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6bcb27e-4557-4f06-944d-613472b5320f",
      "metadata": {
        "id": "b6bcb27e-4557-4f06-944d-613472b5320f"
      },
      "outputs": [],
      "source": [
        "# Shows 9 image examples and their labels\n",
        "class_names = test_ds.class_names\n",
        "print(class_names)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64bf5f25-7b17-459c-a941-e9aed536d933",
      "metadata": {
        "id": "64bf5f25-7b17-459c-a941-e9aed536d933"
      },
      "outputs": [],
      "source": [
        "# Buffered prefetching with automatic parameters for the training, validation, and test\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "\n",
        "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "633e9468-7561-43f5-bdb6-e3d8b1bd0d76",
      "metadata": {
        "id": "633e9468-7561-43f5-bdb6-e3d8b1bd0d76"
      },
      "outputs": [],
      "source": [
        "callback=[\n",
        "    EarlyStopping(monitor='val_loss', patience=10), # Interrupts the training if the validation loss is not lower after 10 epochs\n",
        "    TensorBoard(log_dir=logdir), # Saves training logs in a format used by TensorBoard\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7), # Changes the learning rate in a 0.2 factor every 3 epochs which the validation loss did not decreased until 0.0000001\n",
        "    ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, mode='min', verbose=0) # Saves the best model (based on the lowest validation_loss)\n",
        "]\n",
        "\n",
        "\n",
        "# triangular_cyclical_lr=tfa.optimizers.TriangularCyclicalLearningRate(initial_learning_rate=1e-1, maximal_learning_rate=1e-3, step_size=224, scale_mode='cycle')\n",
        "\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Rescaling(1./255),\n",
        "  layers.Conv2D(64, 3, padding='same', activation=tfa.activations.mish),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(32, 3, padding='same', activation=tfa.activations.mish),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(16, 3, padding='same', activation=tfa.activations.mish),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Conv2D(8, 3, padding='same', activation=tfa.activations.mish),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.MaxPooling2D(),\n",
        "  layers.Flatten(),\n",
        "  layers.Dropout(0.3),\n",
        "  layers.Dense(256, activation=tfa.activations.mish),\n",
        "  layers.BatchNormalization(),\n",
        "  layers.Dropout(0.7),\n",
        "  layers.Dense(3)\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(\n",
        "  optimizer = AdaBeliefOptimizer(learning_rate=1e-3, epsilon=1e-5, rectify=True, print_change_log = False),\n",
        "  loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "  metrics=['accuracy', tfa.metrics.CohenKappa(num_classes=3, sparse_labels=True)]\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "model.fit(\n",
        "  train_ds,\n",
        "  validation_data=val_ds,\n",
        "  callbacks=[callback],\n",
        "  epochs=100\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c83288eb-62cd-46dc-bb8e-5f2f3eadcf8d",
      "metadata": {
        "id": "c83288eb-62cd-46dc-bb8e-5f2f3eadcf8d"
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_ds) # Evaluates the model on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f96ca4ab-9f9c-4cfc-b101-f345503ee1b6",
      "metadata": {
        "id": "f96ca4ab-9f9c-4cfc-b101-f345503ee1b6"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(\"/tf/tcc/model.h5\", custom_objects={\"AdaBeliefOptimizer\": AdaBeliefOptimizer})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4868456-83b6-4203-bb0f-8a974c5c618f",
      "metadata": {
        "id": "b4868456-83b6-4203-bb0f-8a974c5c618f"
      },
      "outputs": [],
      "source": [
        "model.summary() # Shows the model parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e24a54c1-bda5-44ce-a7d4-3ead4dface01",
      "metadata": {
        "id": "e24a54c1-bda5-44ce-a7d4-3ead4dface01"
      },
      "outputs": [],
      "source": [
        "# Generates a Confusion Matrix\n",
        "y_pred = model.predict(test_ds)\n",
        "predicted_categories = tf.argmax(y_pred, axis=1) #y_pred\n",
        "true_categories = tf.concat([y for x, y in test_ds], axis=0) #y_true\n",
        "labels = ['COVID19', 'Normal', 'Pneumonia']\n",
        "\n",
        "# Generates a Normalized Confusion Matrix\n",
        "cm = confusion_matrix(true_categories, predicted_categories, normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
        "disp.plot(cmap=plt.cm.Greens, colorbar=False)\n",
        "plt.title('Normalized Confusion Matrix')\n",
        "plt.savefig(\"ds_normalized_matrix.png\", format=\"png\", dpi=300)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b691e3ac-ef53-4f62-9669-470a27cfcddb",
      "metadata": {
        "id": "b691e3ac-ef53-4f62-9669-470a27cfcddb"
      },
      "outputs": [],
      "source": [
        "# Shows Precision, Recall, F1-Score, Macro e Weighted Average of the model tested on the \"test\" dataset\n",
        "print(classification_report(true_categories, predicted_categories, target_names=labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fe3b39b-c4a2-4f81-b4ea-3ae93835a201",
      "metadata": {
        "id": "5fe3b39b-c4a2-4f81-b4ea-3ae93835a201"
      },
      "outputs": [],
      "source": [
        "# Loads images to be used by the Grad-CAM\n",
        "\n",
        "image1 = \"/dataset/test/Normal/18579_test.png\" \n",
        "image2 = \"/dataset/test/Normal/16324_test.png\" \n",
        "image3 = \"/dataset/test/Normal/18079_test.png\" \n",
        "\n",
        "image4 = \"/dataset/test/Pneumonia/4374_test.png\" \n",
        "image5 = \"/dataset/test/Pneumonia/11444_test.png\"  \n",
        "image6 = \"/dataset/test/Pneumonia/8985_test.png\"  \n",
        "\n",
        "image7 = \"/dataset/test/COVID19/1_test.png\" \n",
        "image8 = \"/dataset/test/COVID19/2158_test.png\"\n",
        "image9 = \"/dataset/test/COVID19/2769_test.png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f14a64-d52f-4fd1-92f2-67a7c5c50b8c",
      "metadata": {
        "id": "e2f14a64-d52f-4fd1-92f2-67a7c5c50b8c"
      },
      "outputs": [],
      "source": [
        "#Neural network intermediate activations visualization, adapted from: https://colab.research.google.com/github/fchollet/deep-learning-with-python-notebooks/blob/master/chapter09_part03_interpreting-what-convnets-learn.ipynb#scrollTo=K9vtVj0iqnB9\n",
        "\n",
        "img_path = image1\n",
        "\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = load_img(\n",
        "    img_path, target_size=target_size)\n",
        "    array = img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "img_tensor = get_img_array(img_path, target_size=(128, 128))\n",
        "\n",
        "\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img_tensor[0].astype(\"uint8\"))\n",
        "plt.show()\n",
        "\n",
        "layer_outputs = []\n",
        "layer_names = []\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, (layers.Conv2D, layers.GlobalMaxPool2D)):\n",
        "        layer_outputs.append(layer.output)\n",
        "        layer_names.append(layer.name)\n",
        "activation_model = keras.Model(inputs=model.input, outputs=layer_outputs)\n",
        "\n",
        "activations = activation_model.predict(img_tensor)\n",
        "first_layer_activation = activations[0]\n",
        "print(first_layer_activation.shape)\n",
        "\n",
        "images_per_row = 8\n",
        "for layer_name, layer_activation in zip(layer_names, activations):\n",
        "    n_features = layer_activation.shape[-1]\n",
        "    size = layer_activation.shape[1]\n",
        "    n_cols = n_features // images_per_row\n",
        "    display_grid = np.zeros(((size + 1) * n_cols - 1,\n",
        "                             images_per_row * (size + 1) - 1))\n",
        "    for col in range(n_cols):\n",
        "        for row in range(images_per_row):\n",
        "            channel_index = col * images_per_row + row\n",
        "            channel_image = layer_activation[0, :, :, channel_index].copy()\n",
        "            if channel_image.sum() != 0:\n",
        "                channel_image -= channel_image.mean()\n",
        "                channel_image /= channel_image.std()\n",
        "                channel_image *= 64\n",
        "                channel_image += 128\n",
        "            channel_image = np.clip(channel_image, 0, 255).astype(\"uint8\")\n",
        "            display_grid[\n",
        "                col * (size + 1): (col + 1) * size + col,\n",
        "                row * (size + 1) : (row + 1) * size + row] = channel_image\n",
        "    scale = 1. / size\n",
        "    plt.figure(dpi=100,figsize=(scale * display_grid.shape[1],\n",
        "                        scale * display_grid.shape[0]))\n",
        "    plt.title(layer_name)\n",
        "    plt.grid(False)\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(display_grid, aspect=\"auto\", cmap=\"viridis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a144f776-dbc5-49b9-a775-7bdd59de9c6b",
      "metadata": {
        "id": "a144f776-dbc5-49b9-a775-7bdd59de9c6b"
      },
      "outputs": [],
      "source": [
        "# Grad-CAM code, adapted from: https://github.com/keras-team/keras-io/blob/master/examples/vision/grad_cam.py\n",
        "\n",
        "\n",
        "img_path = image9\n",
        "size=128\n",
        "\n",
        "# Loads the image and formats it to be received by the model input\n",
        "def get_img_array(img_path, target_size):\n",
        "    img = load_img(\n",
        "    img_path, target_size=target_size)\n",
        "    array = img_to_array(img)\n",
        "    array = np.expand_dims(array, axis=0)\n",
        "    return array\n",
        "\n",
        "img_tensor = get_img_array(img_path, target_size=(128, 128))\n",
        "\n",
        "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
        "    # Creates a new model that maps the last convolution layer activations\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    # The gradient of the class with higher probability is calculated\n",
        "    with tf.GradientTape() as tape:\n",
        "        last_conv_layer_output, preds = grad_model(img_array)\n",
        "        if pred_index is None:\n",
        "            pred_index = tf.argmax(preds[0])\n",
        "        class_channel = preds[:, pred_index]\n",
        "\n",
        "    # Gradient of the output neuron pertaining to the feature map of the last convolutional layer\n",
        "    grads = tape.gradient(class_channel, last_conv_layer_output)\n",
        "\n",
        "    # Vector where each input consists of the average gradient intensity relative to a specific channel of the feature map\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    # Each channel of the activation map is multiplied by the \"degree of importance\" of the channel in relation to the class with the highest probability, and at the end, all channels are summed in order to obtain the heatmap\n",
        "    last_conv_layer_output = last_conv_layer_output[0]\n",
        "    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n",
        "    heatmap = tf.squeeze(heatmap)\n",
        "\n",
        "    # The heatmap scale is normalized between 0 and 1\n",
        "    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n",
        "    return heatmap.numpy()\n",
        "\n",
        "def save_and_display_gradcam(img_path, heatmap, cam_path=\"2769.png\", alpha=0.4):\n",
        "    # Loads the image\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
        "    img = tf.keras.preprocessing.image.img_to_array(img)\n",
        "\n",
        "    # Rescales the heatmap in a scale between 0 and 255\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "\n",
        "    # Uses the \"jet\" colormap for the heatmap\n",
        "    jet = cm.get_cmap(\"jet\")\n",
        "\n",
        "    # Uses RGB values for the colormap/heatmap\n",
        "    jet_colors = jet(np.arange(256))[:, :3]\n",
        "    jet_heatmap = jet_colors[heatmap]\n",
        "\n",
        "    # Generates an image with RGB colorized heatmap\n",
        "    jet_heatmap = tf.keras.preprocessing.image.array_to_img(jet_heatmap)\n",
        "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
        "    jet_heatmap = tf.keras.preprocessing.image.img_to_array(jet_heatmap)\n",
        "\n",
        "    # Superimposes the heatmap to the input image\n",
        "    superimposed_img = jet_heatmap * alpha + img\n",
        "    superimposed_img = tf.keras.preprocessing.image.array_to_img(superimposed_img)\n",
        "\n",
        "    # Saves the image with the superimposed heatmap\n",
        "    superimposed_img.save(cam_path)\n",
        "\n",
        "    # Returns the Grad-CAM result superimposed to the original image\n",
        "    return cam_path\n",
        "\n",
        "last_conv_layer_name = \"conv2d_3\"\n",
        "img_size = (128,128)\n",
        "\n",
        "# Removes the \"softmax\" function from the last layer \n",
        "model.layers[-1].activation = None\n",
        "\n",
        "img_array = img_tensor\n",
        "make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name)\n",
        "cam_path = save_and_display_gradcam(img_path, heatmap)\n",
        "imshow(plt.imread(cam_path))\n",
        "plt.tight_layout()\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "DCNNV_19.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}